{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66199,"databundleVersionId":7522884,"sourceType":"competition"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntraining_set = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train.csv')\nvalidation_set = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation.csv')\ntest_set = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:59:25.281892Z","iopub.execute_input":"2024-03-23T19:59:25.282474Z","iopub.status.idle":"2024-03-23T19:59:26.354380Z","shell.execute_reply.started":"2024-03-23T19:59:25.282447Z","shell.execute_reply":"2024-03-23T19:59:26.353588Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import the os module to interact with the operating system, \n# which allows you to construct file paths and check if files exist.\nimport os\n\n# Define a function that will be used to connect image paths to a pandas DataFrame for training set.\ndef connect_images_to_df_training_set(df):\n    # Define the base path where all your images are stored.\n    # This is the common path shared by all images.\n    base_path = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train/BTTAIxNYBG-train'\n\n    # Create an empty list to store the paths of the images.\n    images = []\n\n    # Iterate over each row in the DataFrame.\n    # 'iterrows()' returns both the index (idx) and the row content (row) for each row in the DataFrame.\n    for idx, row in df.iterrows():\n        # Extract the filename of the image from the 'imageFile' column of the DataFrame.\n        image_name = row['imageFile']\n\n        # Use 'os.path.join' to combine the base path with the image filename.\n        # This creates the full path to the image file.\n        image_path = os.path.join(base_path, image_name)\n\n        # Check if the image file exists at the specified path.\n        if os.path.exists(image_path):\n            # If the file exists, append the path to the 'images' list.\n            images.append(image_path)\n        else:\n            # If the file does not exist, append 'None' to indicate a missing file.\n            # The commented out print statement is useful for debugging to see which files are missing.\n            images.append(None)  # Image not found\n            # print(f\"Image not found for: {image_path}\")  # Debug print\n\n    # Add a new column 'image_path' to the DataFrame.\n    # This column contains the full path to each image.\n    df['image_path'] = images\n    \n# Define a function that will be used to connect image paths to a pandas DataFrame for validation set.\ndef connect_images_to_df_validation_set(df):\n    # Define the base path where all your images are stored.\n    # This is the common path shared by all images.\n    base_path = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation/BTTAIxNYBG-validation'\n\n    # Create an empty list to store the paths of the images.\n    images = []\n\n    # Iterate over each row in the DataFrame.\n    # 'iterrows()' returns both the index (idx) and the row content (row) for each row in the DataFrame.\n    for idx, row in df.iterrows():\n        # Extract the filename of the image from the 'imageFile' column of the DataFrame.\n        image_name = row['imageFile']\n\n        # Use 'os.path.join' to combine the base path with the image filename.\n        # This creates the full path to the image file.\n        image_path = os.path.join(base_path, image_name)\n\n        # Check if the image file exists at the specified path.\n        if os.path.exists(image_path):\n            # If the file exists, append the path to the 'images' list.\n            images.append(image_path)\n        else:\n            # If the file does not exist, append 'None' to indicate a missing file.\n            # The commented out print statement is useful for debugging to see which files are missing.\n            images.append(None)  # Image not found\n            # print(f\"Image not found for: {image_path}\")  # Debug print\n\n    # Add a new column 'image_path' to the DataFrame.\n    # This column contains the full path to each image.\n    df['image_path'] = images\n\n# Define a function that will be used to connect image paths to a pandas DataFrame for test set.\ndef connect_images_to_df_test_set(df):\n    # Define the base path where all your images are stored.\n    # This is the common path shared by all images.\n    base_path = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test/BTTAIxNYBG-test'\n\n    # Create an empty list to store the paths of the images.\n    images = []\n\n    # Iterate over each row in the DataFrame.\n    # 'iterrows()' returns both the index (idx) and the row content (row) for each row in the DataFrame.\n    for idx, row in df.iterrows():\n        # Extract the filename of the image from the 'imageFile' column of the DataFrame.\n        image_name = row['imageFile']\n\n        # Use 'os.path.join' to combine the base path with the image filename.\n        # This creates the full path to the image file.\n        image_path = os.path.join(base_path, image_name)\n\n        # Check if the image file exists at the specified path.\n        if os.path.exists(image_path):\n            # If the file exists, append the path to the 'images' list.\n            images.append(image_path)\n        else:\n            # If the file does not exist, append 'None' to indicate a missing file.\n            # The commented out print statement is useful for debugging to see which files are missing.\n            images.append(None)  # Image not found\n            # print(f\"Image not found for: {image_path}\")  # Debug print\n\n    # Add a new column 'image_path' to the DataFrame.\n    # This column contains the full path to each image.\n    df['image_path'] = images","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:00:10.709490Z","iopub.execute_input":"2024-03-23T20:00:10.710181Z","iopub.status.idle":"2024-03-23T20:00:10.723149Z","shell.execute_reply.started":"2024-03-23T20:00:10.710153Z","shell.execute_reply":"2024-03-23T20:00:10.722068Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"connect_images_to_df_training_set(training_set)\nconnect_images_to_df_validation_set(validation_set)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:00:14.802680Z","iopub.execute_input":"2024-03-23T20:00:14.803004Z","iopub.status.idle":"2024-03-23T20:02:10.945405Z","shell.execute_reply.started":"2024-03-23T20:00:14.802980Z","shell.execute_reply":"2024-03-23T20:02:10.944462Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\nimport math\n\n# Define the number of classes\nnum_classes = 10\n\n# Define image dimensions - changing this makes this take much longer\nimage_height = 224 \nimage_width = 224\n\n# Define batch size\nbatch_size = 32 # experiment with\n\n# Define number of epochs\nnum_epochs = 50\n\nsteps_per_epoch = math.ceil(len(training_set) / batch_size)\nvalidation_steps = math.ceil(len(validation_set) / batch_size)\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nif tf.config.experimental.list_physical_devices('GPU'):\n    print(\"TensorFlow will run on GPU\")\nelse:\n    print(\"TensorFlow will run on CPU by default\")","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:04:14.786540Z","iopub.execute_input":"2024-03-23T20:04:14.787174Z","iopub.status.idle":"2024-03-23T20:04:26.522425Z","shell.execute_reply.started":"2024-03-23T20:04:14.787139Z","shell.execute_reply":"2024-03-23T20:04:26.521462Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-03-23 20:04:16.442129: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-23 20:04:16.442227: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-23 20:04:16.570770: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Num GPUs Available:  2\nTensorFlow will run on GPU\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn import metrics\nfrom tensorflow.keras.applications import ResNet101V2\n\n#TensorFlow Stuff\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.layers as lyrs\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow_hub as hub\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:05:11.076555Z","iopub.execute_input":"2024-03-23T20:05:11.077192Z","iopub.status.idle":"2024-03-23T20:05:12.983547Z","shell.execute_reply.started":"2024-03-23T20:05:11.077160Z","shell.execute_reply":"2024-03-23T20:05:12.982623Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define ImageDataGenerators for training and validation\ntrain_datagen = ImageDataGenerator(rescale=1./255, # only thing here was rescale / added brightness_range and channel_shift_range\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   horizontal_flip=True,\n                                   zoom_range=0.2, \n                                   brightness_range=[0.8,1.2],  \n                                   channel_shift_range=20)  \ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=training_set,  # Use the training_set dataframe\n    directory=None,  # No directory needed since full paths are provided in 'image_path'\n    x_col='image_path',  # Use the 'image_path' column for image paths\n    y_col='classLabel',\n    target_size=(image_height, image_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset = \"training\",\n    seed = 42\n)\n\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_generator = validation_datagen.flow_from_dataframe(\n    dataframe=validation_set,  # Use the validation_set dataframe\n    directory=None,  # No directory needed since full paths are provided in 'image_path'\n    x_col='image_path',  # Use the 'image_path' column for image paths\n    y_col='classLabel',\n    target_size=(image_height, image_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False,\n    seed=42,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:05:16.669324Z","iopub.execute_input":"2024-03-23T20:05:16.670500Z","iopub.status.idle":"2024-03-23T20:05:56.105079Z","shell.execute_reply.started":"2024-03-23T20:05:16.670465Z","shell.execute_reply":"2024-03-23T20:05:56.104344Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found 81946 validated image filenames belonging to 10 classes.\nFound 10244 validated image filenames belonging to 10 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Import Resnet Model\nmodel_base = ResNet101V2(\n    input_shape=(224,224,3),\n    include_top=False,\n)\nmodel_base.trainable = True\n\nfor layers in model_base.layers[:-30]:\n    layers.trainable=False\n\n#Transfer Learning Model\ninputs=tf.keras.Input(shape=(224,224,3))\nx=model_base(inputs)\nx=lyrs.GlobalAveragePooling2D()(x)\nx= lyrs.Dropout(0.5)(x)\noutputs=lyrs.Dense(10, activation=\"softmax\", \n                     kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:10:41.990159Z","iopub.execute_input":"2024-03-23T20:10:41.990820Z","iopub.status.idle":"2024-03-23T20:10:46.317585Z","shell.execute_reply.started":"2024-03-23T20:10:41.990790Z","shell.execute_reply":"2024-03-23T20:10:46.316502Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m171317808/171317808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ resnet101v2 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m42,626,560\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m20,490\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ resnet101v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">42,626,560</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m42,647,050\u001b[0m (162.69 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,647,050</span> (162.69 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,464,010\u001b[0m (55.18 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,464,010</span> (55.18 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m28,183,040\u001b[0m (107.51 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,183,040</span> (107.51 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"checkpointer = ModelCheckpoint('best_model.keras',\n                               verbose=1, \n                               save_best_only= True, \n                               monitor='val_accuracy',  \n                               mode='max'  # Ensure mode is set to 'max' since you're looking to maximize accuracy\n                              )\nearly_stopping = EarlyStopping(monitor= 'val_accuracy', patience= 6)\n# verbose is 3 instead of 6 because of the bug where every other epoch gives no accuracy or loss\n\nmodel.compile(loss=\"categorical_crossentropy\", \n              optimizer=Adam(learning_rate=1e-4), \n              metrics=[\"accuracy\"])\nmodel_history=model.fit(x=train_generator,  # Use train_generator here\n    steps_per_epoch=steps_per_epoch,\n    validation_data=validation_generator,  # Use validation_generator here\n    validation_steps=validation_steps,  # Ensure you define validation_steps\n    epochs=num_epochs,\n    callbacks=[checkpointer, early_stopping])","metadata":{"execution":{"iopub.status.busy":"2024-03-24T00:56:57.847797Z","iopub.execute_input":"2024-03-24T00:56:57.848814Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m   1/2561\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:55:59\u001b[0m 31s/step - accuracy: 1.0000 - loss: 0.0155","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711241849.355228      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1094/2561\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m18:25\u001b[0m 753ms/step - accuracy: 0.9761 - loss: 0.0748","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711242672.890352      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751ms/step - accuracy: 0.9762 - loss: 0.0746","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711243779.720113      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_accuracy improved from -inf to 0.97306, saving model to best_model.keras\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711243866.024047      87 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2049s\u001b[0m 788ms/step - accuracy: 0.9762 - loss: 0.0746 - val_accuracy: 0.9731 - val_loss: 0.1048\nEpoch 2/50\n\nEpoch 2: val_accuracy did not improve from 0.97306\n\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1124/2561\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m16:27\u001b[0m 687ms/step - accuracy: 0.9783 - loss: 0.0674","output_type":"stream"}]},{"cell_type":"code","source":"connect_images_to_df_test_set(test_set)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T00:39:19.341684Z","iopub.execute_input":"2024-03-24T00:39:19.342076Z","iopub.status.idle":"2024-03-24T00:39:55.925132Z","shell.execute_reply.started":"2024-03-24T00:39:19.342045Z","shell.execute_reply":"2024-03-24T00:39:55.924137Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import load_model\n\n# Define ImageDataGenerator for preprocessing\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create a test generator\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=test_set,\n    directory=None,  \n    x_col='image_path',  # Column in test_set that contains the image paths\n    y_col=None,  # No labels\n    target_size=(image_height, image_width),  # Same as training/validation\n    batch_size=batch_size,  \n    class_mode=None,  # Since we're predicting\n    shuffle=False  # Important for keeping track of filenames\n)\n\n# Load the best model saved by the ModelCheckpoint\nbest_model = load_model('best_model.keras')\n\n# Now use this best_model to make predictions\npredictions = best_model.predict(test_generator, steps=int(np.ceil(len(test_set)/batch_size)))\npredicted_classes = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n\n# Create a DataFrame with 'uniqueID' and predicted 'classID'\nsubmission_df = pd.DataFrame({\n    'uniqueID': test_set['uniqueID'],  # Assuming 'uniqueID' is a column in test_set\n    'classID': predicted_classes\n})\n\n# Ensure the DataFrame matches the format of the sample submission\nsubmission_df = submission_df.reset_index(drop=True)\n\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T00:44:13.828463Z","iopub.execute_input":"2024-03-24T00:44:13.829167Z","iopub.status.idle":"2024-03-24T00:51:27.026126Z","shell.execute_reply.started":"2024-03-24T00:44:13.829134Z","shell.execute_reply":"2024-03-24T00:51:27.025024Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Found 30690 validated image filenames.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 690 variables whereas the saved optimizer has 64 variables. \n  trackable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  2/960\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 124ms/step","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711241100.656967      88 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 402ms/step\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711241486.427997      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   uniqueID  classID\n0         1        1\n1         9        7\n2        10        4\n3        14        1\n4        16        6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uniqueID</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T00:52:52.127450Z","iopub.execute_input":"2024-03-24T00:52:52.128326Z","iopub.status.idle":"2024-03-24T00:52:52.180381Z","shell.execute_reply.started":"2024-03-24T00:52:52.128292Z","shell.execute_reply":"2024-03-24T00:52:52.179470Z"},"trusted":true},"execution_count":13,"outputs":[]}]}